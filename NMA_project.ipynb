{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyW9c6vEXR5PXqt5qpoknS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XZhangNH/XZhangNH/blob/Projects/neuromatch_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm8QoPRoStgj"
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pkg_resources\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(action='once')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiR5Uv4OZ9qw",
        "outputId": "7a41024d-6be8-42ce-f1a4-591bef19ac46"
      },
      "source": [
        "!git clone --recurse-submodules https://github.com/ColeLab/ActflowToolbox.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ActflowToolbox' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-9dA04QapVP",
        "outputId": "766039d5-28cc-4255-af98-a6863dd6102a"
      },
      "source": [
        "!pip install nltools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/07/7332b07d8cd20f0d22cd561d03ebdc30801d85572d7e8c79c6a815768e11/nltools-0.4.4-py2.py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 4.9MB/s \n",
            "\u001b[?25hCollecting pynv\n",
            "  Downloading https://files.pythonhosted.org/packages/05/c0/73cd8f13191d439ddf60ceda3be50a5e972e3b032ffd0bffcfff1f0c50f1/pynv-0.2-py3-none-any.whl\n",
            "Requirement already satisfied: seaborn>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from nltools) (0.11.1)\n",
            "Collecting deepdish>=0.3.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/39/2a47c852651982bc5eb39212ac110284dd20126bdc7b49bde401a0139f5d/deepdish-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.7/dist-packages (from nltools) (1.0.1)\n",
            "Collecting nilearn>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/03/54010b2bbbf0e784ee11ca0d25bd644dba05e618d876f7fb8fdeb8eafaa0/nilearn-0.8.0-py3-none-any.whl (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 44.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from nltools) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.7/dist-packages (from nltools) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from nltools) (0.22.2.post1)\n",
            "Requirement already satisfied: nibabel>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from nltools) (3.0.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from nltools) (3.2.2)\n",
            "Requirement already satisfied: pandas<1.2,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from nltools) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from pynv->nltools) (2.23.0)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from deepdish>=0.3.6->nltools) (3.4.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.0->nltools) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.0->nltools) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.0->nltools) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.0->nltools) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<1.2,>=1.1.0->nltools) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->pynv->nltools) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->pynv->nltools) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->pynv->nltools) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->pynv->nltools) (3.0.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from tables->deepdish>=0.3.6->nltools) (1.15.0)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->deepdish>=0.3.6->nltools) (2.7.3)\n",
            "Installing collected packages: pynv, deepdish, nilearn, nltools\n",
            "Successfully installed deepdish-0.3.6 nilearn-0.8.0 nltools-0.4.4 pynv-0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/deepdish-0.3.6.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/nilearn-0.8.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/nltools-0.4.4.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/pynv-0.2.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIrYJz6PW99A",
        "outputId": "0648006d-79ad-4386-b0e3-3431edbe909f"
      },
      "source": [
        "import ActflowToolbox as actflow\n",
        "from nltools.utils import get_resource_path\n",
        "from nltools.file_reader import onsets_to_dm\n",
        "from nltools.data import Design_Matrix\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import scale"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py:1131: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n",
            "  self, resource_name\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/local/lib/python3.7/dist-packages/patsy/constraint.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Mapping\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
            "  \"Numpy arrays.\", FutureWarning)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5n2ENA1WuzI"
      },
      "source": [
        "import urllib.request\n",
        "import tarfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV9VbBAmYaKS"
      },
      "source": [
        "thetarfile = \"https://osf.io/s4h8j/download/\"\n",
        "ftpstream = urllib.request.urlopen(thetarfile)\n",
        "thetarfile = tarfile.open(fileobj=ftpstream, mode=\"r|gz\")\n",
        "thetarfile.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U0OFOwcXDG2"
      },
      "source": [
        "thetarfile = \"https://osf.io/bqp7m/download/\"\n",
        "ftpstream = urllib.request.urlopen(thetarfile)\n",
        "thetarfile = tarfile.open(fileobj=ftpstream, mode=\"r|gz\")\n",
        "thetarfile.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJgosKLFbd4c",
        "outputId": "518e0294-0365-4548-d5c6-98a10934bc4c"
      },
      "source": [
        "fname = \"hcp_task.tgz\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/2y3fw/download\n",
        "  !tar -xzf $fname -C $HCP_DIR --strip-components=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar (child): -C: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv18WJ1KXQSS"
      },
      "source": [
        "# The download cells will store the data in nested directories starting here:\n",
        "HCP_DIR = \"./hcp\"\n",
        "if not os.path.isdir(HCP_DIR):\n",
        "  os.mkdir(HCP_DIR)\n",
        "  \n",
        "# The data shared for NMA projects is a subset of the full HCP dataset\n",
        "N_SUBJECTS = 339\n",
        "\n",
        "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
        "N_PARCELS = 360\n",
        "\n",
        "# The acquisition parameters for all tasks were identical\n",
        "TR = 0.72  # Time resolution, in sec\n",
        "\n",
        "# The parcels are matched across hemispheres with the same order\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "\n",
        "# Each experiment was repeated multiple times in each subject\n",
        "N_RUNS_REST = 4\n",
        "N_RUNS_TASK = 2\n",
        "\n",
        "# Time series data are organized by experiment, with each experiment\n",
        "# having an LR and RL (phase-encode direction) acquistion\n",
        "BOLD_NAMES = [\n",
        "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
        "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
        "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
        "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
        "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
        "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
        "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
        "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
        "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "#Dictionaries \n",
        "conditions_dict={\n",
        "    \"motor\": [\"cue\", \"rf\", \"lf\", \"rh\", \"lh\"],\n",
        "    \"wm\": [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\", \"2bk_body\", \n",
        "           \"2bk_faces\", \"2bk_places\", \"2bk_tools\"],\n",
        "    \"emotion\": [\"fear\", \"neut\"],\n",
        "    \"gambling\": [\"win\", \"loss\"],\n",
        "    \"language\": [\"story\", \"math\"],\n",
        "    \"relational\": [\"match\", \"relation\"],\n",
        "    \"social\": [\"mental\", \"rnd\"]}\n",
        "\n",
        "run_length_dict = {\n",
        "    \"motor\": 284,\n",
        "    \"wm\": 405,\n",
        "    \"emotion\": 176,\n",
        "    \"gambling\": 253,\n",
        "    \"language\": 316,\n",
        "    \"relational\": 232,\n",
        "    \"social\": 274}\n",
        "\n",
        "bold_name_dict = {\n",
        "    \"rest\": [\"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\", \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\"],\n",
        "    \"motor\": [\"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\"],\n",
        "    \"wm\": [\"tfMRI_WM_RL\", \"tfMRI_WM_LR\"],\n",
        "    \"emotion\": [\"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\"],\n",
        "    \"gambling\": [\"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\"],\n",
        "    \"language\": [\"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\"],\n",
        "    \"relational\": [\"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\"],\n",
        "    \"social\": [\"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"]}\n",
        "\n",
        "task_run_dict = {\n",
        "    \"rest\": [1,2,3,4],\n",
        "    \"motor\": [5,6],\n",
        "    \"wm\": [7,8],\n",
        "    \"emotion\": [9,10],\n",
        "    \"gambling\": [11,12],\n",
        "    \"language\": [13, 14],\n",
        "    \"relational\": [15, 16],\n",
        "    \"social\": [17, 18]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlewkRT4XTys"
      },
      "source": [
        "def get_cond_evs(cond_name, task_name, subject, run = 1):\n",
        "  \"\"\"Load onset files for a single condition from a task.\n",
        "  \n",
        "  Args:\n",
        "    cond_name (str): condition name pulled from the conditions_dict for task_name\n",
        "    task_name (str): task name instead of the bold run\n",
        "    subject (int): 0-based subject ID to load\n",
        "    run (int): 1 or 2 for task runs (use run-1 for indexing)\n",
        "\n",
        "  Returns\n",
        "    cond_evs (n_blocks x 3): Events file for single condition to be formatted for design matrix\n",
        "\n",
        "  \"\"\"\n",
        "  bold_name = bold_name_dict[task_name][run-1]\n",
        "  cond_evs = pd.read_csv('%s/subjects/%s/%s/%s/EVs/%s.txt'%(HCP_DIR, subject, EXPERIMENTS, bold_name, cond_name), sep=\"\\t\", header=None)\n",
        "  cond_evs = cond_evs.rename(columns={0: \"Onset\", 1: \"Duration\", 2: \"amplitude\"})\n",
        "  cond_evs = cond_evs.drop(columns=['amplitude'])\n",
        "  cond_evs['Stim'] = cond_name\n",
        "\n",
        "  return cond_evs\n",
        "\n",
        "def get_run_evs(subject, task_name, run = 1):\n",
        "  \"\"\"Load onset files for a full file.\n",
        "  \n",
        "  Args:\n",
        "    task_name (str): task name instead of the bold run\n",
        "    subject (int): 0-based subject ID to load\n",
        "    run (int): 1 or 2 for task runs (use run-1 for indexing)\n",
        "\n",
        "  Returns\n",
        "    evs (n_blocks for run x 3 array): Events file for single condition to be formatted for design matrix\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  conditions = conditions_dict[task_name]\n",
        "\n",
        "  evs = pd.DataFrame()\n",
        "\n",
        "  for cond in conditions:\n",
        "    cond_evs = get_cond_evs(cond, task_name, subject, run)\n",
        "    evs = evs.append(cond_evs)\n",
        "  \n",
        "  evs = evs.sort_values(by=\"Onset\") \n",
        "\n",
        "  return evs\n",
        "\n",
        "def run_evs_to_dm(run_evs, task_name, TR=.72, convolve = True, add_poly = 2, dct_basis=False):\n",
        "\n",
        "  sampling_freq = 1./TR\n",
        "  run_length = run_length_dict[task_name]\n",
        "  dm = onsets_to_dm(run_evs, sampling_freq=sampling_freq, run_length=run_length, sort=True, add_poly=add_poly)\n",
        "\n",
        "  if convolve: \n",
        "    dm = dm.convolve()\n",
        "\n",
        "  if dct_basis:\n",
        "    dm = dm.add_dct_basis()\n",
        "\n",
        "  return dm\n",
        "\n",
        "def get_task_dms(subject, task_name, TR = .72, convolve = True, add_poly = 2, dct_basis=False):\n",
        "\n",
        "  runs = list(range(1,len(task_run_dict[task_name])+1))\n",
        "  task_dm = Design_Matrix(sampling_freq=1./TR)\n",
        "\n",
        "  for run in runs:\n",
        "    run_evs = get_run_evs(subject=subject, task_name=task_name, run=run)\n",
        "    run_dm = run_evs_to_dm(run_evs=run_evs, task_name=task_name, add_poly=add_poly, dct_basis=dct_basis)\n",
        "    task_dm = task_dm.append(run_dm)\n",
        "\n",
        "  return task_dm\n",
        "\n",
        "def load_run_timeseries(subject, task_name, run = 1, remove_mean=True, scale_ts=True):\n",
        "  \"\"\"Load timeseries data for a single subject and single run.\n",
        "  \n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    task_name (str): task name instead of the bold run\n",
        "    run (int): 1 or 2 for task runs\n",
        "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  bold_run = task_run_dict[task_name][run-1]\n",
        "\n",
        "  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n",
        "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
        "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
        "  if remove_mean:\n",
        "    ts -= ts.mean(axis=1, keepdims=True)\n",
        "\n",
        "  if scale_ts:\n",
        "    #scales each parcel's timeseries (instead of scaling the bold for one 1 TR from all parcels)\n",
        "    ts = scale(ts, axis=1)\n",
        "  return ts\n",
        "\n",
        "def load_task_timeseries(subject, task_name, remove_mean=True, scale_ts = True):\n",
        "  \n",
        "  runs = list(range(1,len(task_run_dict[task_name])+1))\n",
        "  task_ts = np.empty((360, 0))\n",
        "\n",
        "  for run in runs:\n",
        "    #since everything is loaded by run and scale_ts is true each parcel should be \n",
        "    #scaled for each parcel and for each run separately before being concatenated together\n",
        "    cur_run_ts = load_run_timeseries(subject=subject, task_name=task_name, run=run)\n",
        "    task_ts = np.append(task_ts, cur_run_ts, axis=1)\n",
        "  \n",
        "  return task_ts\n",
        "\n",
        "def get_sub_task_resids(subject, task_name):\n",
        " \n",
        "  #load task data\n",
        "  task_ts = load_task_timeseries(subject=subject, task_name=task_name)\n",
        "\n",
        "  #make design matrix\n",
        "  task_dm = get_task_dms(subject=subject, task_name=task_name)\n",
        "\n",
        "  #initialize empty variables to store data in\n",
        "  run_length = run_length_dict[task_name]\n",
        "  num_runs = len(task_run_dict[task_name])\n",
        "  resids = np.empty((0, num_runs*run_length))\n",
        "\n",
        "  #loop through parcels, run regression and extract residuals\n",
        "  for parcel in range(len(task_ts)):\n",
        "    model = sm.OLS(task_ts[parcel], task_dm)\n",
        "    results = model.fit()\n",
        "    cur_resids = np.array([results.resid])\n",
        "    resids = np.append(resids, cur_resids, axis=0)\n",
        "\n",
        "  #store parcel residuals in same format as original BOLD\n",
        "  out_dir = './hcp/residuals/%s'%(task_name)\n",
        "  if not os.path.isdir(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "  out_fn = '%s_%s_Glasser360Cortical.npy'%(task_name, str(subject))\n",
        "\n",
        "  np.save(os.path.join(out_dir, out_fn), resids)\n",
        "\n",
        "  return resids\n",
        "\n",
        "def load_fcs(task_name, fc_type):\n",
        "  \n",
        "  base_dir = '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "  if fc_type == \"resid\":\n",
        "    fc_dir = os.path.join(base_dir, 'residual_fcs')\n",
        "  \n",
        "  elif fc_type == \"task\":\n",
        "    fc_dir = os.path.join(base_dir, 'task_preds_fcs')\n",
        "\n",
        "  elif fc_type == \"rest\":\n",
        "    fc_dir = os.path.join(base_dir, 'rest_fcs')\n",
        "\n",
        "  input_dir = os.path.join(fc_dir, task_name)\n",
        "  fcs_list = os.listdir(input_dir)\n",
        "  fcs = np.zeros((360, 360, len(fcs_list)))\n",
        "\n",
        "  for i, fc in enumerate(fcs_list):\n",
        "    fcs[:,:,i] = np.load(os.path.join(input_dir, fc))\n",
        "\n",
        "  return fcs\n",
        "\n",
        "def get_sub_task_pred(subject, task_name):\n",
        " \n",
        "  #load task data\n",
        "  task_ts = load_task_timeseries(subject=subject, task_name=task_name)\n",
        "\n",
        "  #make design matrix\n",
        "  task_dm = get_task_dms(subject=subject, task_name=task_name)\n",
        "  task_regs = task_dm.iloc[:,:len(conditions_dict[task_name])]\n",
        "\n",
        "  #initialize empty variables to store data in\n",
        "  run_length = run_length_dict[task_name]\n",
        "  num_runs = len(task_run_dict[task_name])\n",
        "  preds = np.empty((0, num_runs*run_length))\n",
        "\n",
        "  #loop through parcels, run regression and extract residuals\n",
        "  for parcel in range(len(task_ts)):\n",
        "    model = sm.OLS(task_ts[parcel], task_dm)\n",
        "    results = model.fit()\n",
        "    task_coefs = results.params[:len(conditions_dict[task_name])]\n",
        "    cur_preds = np.zeros(num_runs*run_length)\n",
        "    for i in range(len(conditions_dict['emotion'])):\n",
        "      cur_preds += task_coefs[i]*task_regs.iloc[:,i]\n",
        "    cur_preds = np.array(cur_preds).reshape(1, -1)\n",
        "    preds = np.append(preds, cur_preds, axis=0)\n",
        "\n",
        "  #store parcel residuals in same format as original BOLD\n",
        "  out_dir = './hcp/task_preds/%s'%(task_name)\n",
        "  if not os.path.isdir(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "  out_fn = '%s_%s_Glasser360Cortical.npy'%(task_name, str(subject))\n",
        "\n",
        "  np.save(os.path.join(out_dir, out_fn), preds)\n",
        "\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN6s_f5IXWNk"
      },
      "source": [
        "regions = np.load('./hcp_task/regions.npy').T\n",
        "region_info = dict(\n",
        "    name=regions[0].tolist(),\n",
        "    network=regions[1],\n",
        "    myelin=regions[2].astype(np.float),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDhrdGnBXXwD"
      },
      "source": [
        "networkpartition_dir = pkg_resources.resource_filename('ActflowToolbox.dependencies', 'ColeAnticevicNetPartition/')\n",
        "networkdef = np.loadtxt(networkpartition_dir + '/cortex_parcel_network_assignments.txt')\n",
        "networkorder = np.asarray(sorted(range(len(networkdef)), key=lambda k: networkdef[k]))\n",
        "networkorder.shape = (len(networkorder),1)\n",
        "netorder=networkorder[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA4Ef6HkXZYP"
      },
      "source": [
        "network_dict = {1: 'Visual1', \n",
        "       2: 'Visual2', \n",
        "       3:'Somatomotor',\n",
        "       4:'Cingulo-Oper',\n",
        "       5:'Language',\n",
        "       6:'Default',\n",
        "       7:'Frontopariet',\n",
        "       8:'Auditory',\n",
        "       9:'Posterior-Mu',\n",
        "       10:'Dorsal-atten',\n",
        "       11:'Ventral-Mult',\n",
        "       12:'Orbito-Affec'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFMZ_KcLa1Tr"
      },
      "source": [
        "net_col_dict = {'Visual1': (0, 0, 1),\n",
        "                'Visual2':(0.3922, 0, 1),\n",
        "                'Somatomotor':(0, 1, 1),\n",
        "                'Cingulo-Oper':(0.6, 0, 0.6),\n",
        "                'Language':(0, 1, 0),\n",
        "                'Default':(0, 0.6, 0.6),\n",
        "                'Frontopariet':(1, 1, 0),\n",
        "                'Auditory':(0.98, 0.24, 0.98),\n",
        "                'Posterior-Mu':(1, 0, 0),\n",
        "                'Dorsal-atten':(0.7, 0.35, 0.16),\n",
        "                'Ventral-Mult':(1, 0.6, 0),\n",
        "                'Orbito-Affec':(0.25, 0.5, 0)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PxqppGlXbFT"
      },
      "source": [
        "subject = 100307\n",
        "task_name = 'relational'\n",
        "run = 1\n",
        "EXPERIMENTS = 'RELATIONAL'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "k0gHA-EmXdPH",
        "outputId": "e5e7e7b2-4bb4-4395-b639-7d2f47b8c94d"
      },
      "source": [
        "run_evs = get_run_evs(subject=subject, task_name = task_name, run = run)\n",
        "run_evs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0d4d47b9bfb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_evs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_run_evs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrun_evs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b1504048d202>\u001b[0m in \u001b[0;36mget_run_evs\u001b[0;34m(subject, task_name, run)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconditions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mcond_evs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cond_evs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_evs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b1504048d202>\u001b[0m in \u001b[0;36mget_cond_evs\u001b[0;34m(cond_name, task_name, subject, run)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \"\"\"\n\u001b[1;32m     14\u001b[0m   \u001b[0mbold_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbold_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mcond_evs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/subjects/%s/%s/%s/EVs/%s.txt'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHCP_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEXPERIMENTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mcond_evs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_evs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Onset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Duration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"amplitude\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mcond_evs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_evs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amplitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './hcp/subjects/100307/RELATIONAL/tfMRI_RELATIONAL_RL/EVs/match.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nibnJ-P-Xik6"
      },
      "source": [
        "dm = run_evs_to_dm(run_evs, task_name = task_name)\n",
        "dm.heatmap()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOJQNOP-Xj_-"
      },
      "source": [
        "dm_c = dm.convolve()\n",
        "dm_c.heatmap()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-2Wm1OcXlaU"
      },
      "source": [
        "bold_data = load_run_timeseries(subject=subject, task_name=task_name, run=run)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DaMT4xUXm2F"
      },
      "source": [
        "plt.plot(scale(bold_data[0]), label = \"Scaled BOLD from parcel 0\")\n",
        "plt.plot(dm_c['fear_c0'], label = \"Fear condition regressor\")\n",
        "plt.plot(dm_c['neut_c0'], label = \"Neut condition regressor\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbbAX9dzXonJ"
      },
      "source": [
        "mod = sm.OLS(scale(bold_data[0]), dm_c)\n",
        "res = mod.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDlqlISxXqGt"
      },
      "source": [
        "res.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
